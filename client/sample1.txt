The process begins with the leaf nodes containing the probabilities of the symbol they represent. Then, the process takes the two nodes 
with smallest probability, and creates a new internal node having these two nodes as children. The weight of the new node is set to the sum 
of the weight of the children. We then apply the process again, on the new internal node and on the remaining nodes (i.e., we exclude the 
two leaf nodes), we repeat this process until only one node remains, which is the root of the Huffman tree.
The simplest construction algorithm uses a priority queue where the node with lowest probability is given highest priority:
Create a leaf node for each symbol and add it to the priority queue.While there is more than one node in the queue:
Remove the two nodes of highest priority (lowest probability) from the queue
Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes' probabilities.
Add the new node to the queue.
The remaining node is the root node and the tree is complete.
Since efficient priority queue data structures require O(log n) time per insertion, and a tree with n leaves has 2nâˆ’1 nodes, this algorithm 
operates in O(n log n) time, where n is the number of symbols.
If the symbols are sorted by probability, there is a linear-time (O(n)) method to create a Huffman tree using two queues, the first one 
containing the initial weights (along with pointers to the associated leaves), and combined weights (along with pointers to the trees) 
being put in the back of the second queue. This assures that the lowest weight is always kept at the front of one of the two queues: 
