In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless 
data compression. The process of finding or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. 
Huffman while he was a Sc.D. student at MIT, and published in the 1952 paper "A Method for the Construction of Minimum-Redundancy Codes".[1]
The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a 
file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the 
source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common 
symbols. Huffman's method can be efficiently implemented, finding a code in time linear to the number of input weights if these weights are 
sorted.[2] However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression 
methods.
Huffman coding uses a specific method for choosing the representation for each symbol, resulting in a prefix code (sometimes called "prefix-
free codes", that is, the bit string representing some particular symbol is never a prefix of the bit string representing any other 
symbol). Huffman coding is such a widespread method for creating prefix codes that the term "Huffman code" is widely used as a synonym for 
"prefix code" even when such a code is not produced by Huffman's algorithm.
The technique works by creating a binary tree of nodes. These can be stored in a regular array, the size of which depends on the number of 
symbols, n {\displaystyle n} n. A node can be either a leaf node or an internal node. Initially, all nodes are leaf nodes, which contain 
the symbol itself, the weight (frequency of appearance) of the symbol and optionally, a link to a parent node which makes it easy to read 
the code (in reverse) starting from a leaf node. Internal nodes contain a weight, links to two child nodes and an optional link to a parent 
node. As a common convention, bit '0' represents following the left child and bit '1' represents following the right child. A finished tree 
has up to n {\displaystyle n} n leaf nodes and n âˆ’ 1 {\displaystyle n-1} n-1 internal nodes. A Huffman tree that omits unused symbols 
produces the most optimal code lengths.